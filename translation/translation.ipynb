{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.6\n",
    "VAL_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "VOCAB_SIZE = 10000\n",
    "MODEL_DIM = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind machine translation is to take a piece of text in one language and translate it to another language. In this example, we will look at translating English text into Welsh text.\n",
    "\n",
    "In order to validate our approach, we need examples of English text that have been (correctly) translated to the corresponding Welsh."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is generally too large to load entirely into memory, therefore we need to be able to load batches of translations dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnCyCorpus(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, en_corpus_file, cy_corpus_file):\n",
    "        super(EnCyCorpus, self).__init__()\n",
    "        self.en_corpus_file = en_corpus_file\n",
    "        self.cy_corpus_file = cy_corpus_file\n",
    "\n",
    "    def tidy(self, text):\n",
    "        return text\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create an iterator\n",
    "        en_itr = open(self.en_corpus_file)\n",
    "        cy_itr = open(self.cy_corpus_file)\n",
    "        \n",
    "        # Map each element using the line_mapper\n",
    "        mapped_en_itr = map(self.tidy, en_itr)\n",
    "        mapped_cy_itr = map(self.tidy, cy_itr)\n",
    "        \n",
    "        # Zip both iterators\n",
    "        zipped_itr = zip(mapped_en_itr, mapped_cy_itr)\n",
    "        \n",
    "        return zipped_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = EnCyCorpus('translation/data/CofnodBachYCynulliad/CofnodBachYCynulliad.en',\n",
    "                  'translation/data/CofnodBachYCynulliad/CofnodBachYCynulliad.cy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    # Load texts\n",
    "    with open(\"translation/data/CofnodYCynulliad/CofnodYCynulliad.en\", 'r', encoding='utf-8') as f:\n",
    "        english_texts = f.read().splitlines()\n",
    "    with open(\"translation/data/CofnodYCynulliad/CofnodYCynulliad.cy\", 'r', encoding='utf-8') as f:\n",
    "        welsh_texts = f.read().splitlines()\n",
    "    texts = list(zip(english_texts, welsh_texts))\n",
    "\n",
    "    # Split into train / val / test\n",
    "    n_texts = len(texts)\n",
    "    corpus = {}\n",
    "    corpus['train'] = texts[:round(n_texts * TRAIN_PERC)]\n",
    "    corpus['val'] = texts[round(n_texts * TRAIN_PERC):(round(n_texts * TRAIN_PERC) + round(n_texts * VAL_PERC))]\n",
    "    corpus['test'] = texts[-round(n_texts * TEST_PERC):]\n",
    "\n",
    "    # Sort by length to help with batching\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i hope that that will help create the right atmosphere when answering questions .',\n",
       "  \"gobeithiaf y bydd hynny o gymorth i greu'r awyrgylch iawn wrth ateb cwestiynau .\"),\n",
       " ('extensive consultation was carried out with every relevant party , and the replies are being analysed at present .',\n",
       "  'cafwyd ymgynghori helaeth gyda phob sefydliad perthnasol a dadansoddir yr atebion ar hyn o bryd .')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = corpus['train'][1000:1002]\n",
    "example_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs the following as input:\n",
    "1. The English text, converted into token indices\n",
    "2. The output Welsh text, converted into token indices\n",
    "3. The decoder input Welsh text, converted into token indices, which is a shifted version of the output.\n",
    "4. An attention mask for the English text\n",
    "5. An attention mask for the Welsh text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing the text is to break it up into separate tokens, and assign an index to each token. These are typically sub-word level pieces of text. Ideally, we would do this separately for both English and Welsh, since they clearly have different atomic tokens, however for ease of use we will create one tokenizer to deal with both.\n",
    "\n",
    "https://huggingface.co/course/chapter6/8?fw=pt#building-a-bpe-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import models, pre_tokenizers, trainers, processors\n",
    "from tokenizers import normalizers\n",
    "from tokenizers import decoders\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text):\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    "    )\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    "    )\n",
    "    special_tokens = [\"[BOS]\", \"[EOS]\", \"[PAD]\", \"[MASK]\", \"[UNK]\"]\n",
    "    tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "    tokenizer.train_from_iterator(text, trainer)\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[BOS] $A [EOS]\",\n",
    "        special_tokens=[\n",
    "            (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "            (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "    )\n",
    "    return pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_tokenizer = create_tokenizer(\n",
    "    text=[pair[0] for pair in corpus['train']]\n",
    ")\n",
    "welsh_tokenizer = create_tokenizer(\n",
    "    text=[pair[1] for pair in corpus['train']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "example_tokenizer_output = english_tokenizer(\n",
    "    text=[ex[0] for ex in example_text],\n",
    "    return_token_type_ids=False, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "         5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "        [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "          112, 7402,  170,  583, 9528,   90,  246, 1028,   18,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenizer_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we need to deal with batches of input, and for training the model it is more efficient. The input to the model is:\n",
    "1. All the English tokens\n",
    "2. All the Welsh tokens\n",
    "3. The source attention mask to tell it which source tokens to pay attention to\n",
    "4. The target attention mask to tell it which target tokens to pay attention to\n",
    "5. The decoder input ids, which are right-shifted target tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(texts):\n",
    "    src_batch = english_tokenizer(\n",
    "        text=[text[0] for text in texts],\n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    src_batch = {'src_' + str(k): v for k,v in src_batch.items()}\n",
    "    tgt_batch = welsh_tokenizer(\n",
    "        text=[text[1] for text in texts],\n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    tgt_batch = {'tgt_' + str(k): v for k,v in tgt_batch.items()}\n",
    "    tgt_batch['tgt_output_ids'] = tgt_batch['tgt_input_ids'][:, 1:]\n",
    "    tgt_batch['tgt_input_ids'] = tgt_batch['tgt_input_ids'][:, :-1]\n",
    "    tgt_batch['tgt_attention_mask'] = tgt_batch['tgt_attention_mask'][:, :-1]\n",
    "\n",
    "    # Extend shortest input\n",
    "    src_shape = src_batch['src_input_ids'].shape\n",
    "    tgt_shape = tgt_batch['tgt_input_ids'].shape\n",
    "    if src_shape[1] < tgt_shape[1]:\n",
    "        diff = tgt_shape[1] - src_shape[1]\n",
    "        src_batch['src_input_ids'] = torch.cat((src_batch['src_input_ids'], torch.full([src_shape[0],diff], 2)), dim=1)\n",
    "        src_batch['src_attention_mask'] = torch.cat((src_batch['src_attention_mask'], torch.full([src_shape[0],diff], 0)), dim=1)\n",
    "    elif tgt_shape[1] < src_shape[1]:\n",
    "        diff = src_shape[1] - tgt_shape[1]\n",
    "        tgt_batch['tgt_input_ids'] = torch.cat((tgt_batch['tgt_input_ids'], torch.full([tgt_shape[0],diff], 2)), dim=1)\n",
    "        tgt_batch['tgt_attention_mask'] = torch.cat((tgt_batch['tgt_attention_mask'], torch.full([tgt_shape[0],diff], 0)), dim=1)\n",
    "        tgt_batch['tgt_output_ids'] = torch.cat((tgt_batch['tgt_output_ids'], torch.full([tgt_shape[0],diff], 2)), dim=1)\n",
    "    \n",
    "    # Combine\n",
    "    batch = {**src_batch, **tgt_batch}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "example_batch = collate_batch(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "          5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "         [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "           112, 7402,  170,  583, 9528,   90,  246, 1028,   18,    1]]),\n",
       " 'src_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'tgt_input_ids': tensor([[   0, 1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,\n",
       "           583,  326,  682, 1735,   18,    1,    2,    2,    2,    2],\n",
       "         [   0, 2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   81,  153,\n",
       "          2502,  132,  161,   53,  558,   18,    2,    2,    2,    2]]),\n",
       " 'tgt_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " 'tgt_output_ids': tensor([[1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,  583,\n",
       "           326,  682, 1735,   18,    1,    2,    2,    2,    2,    2],\n",
       "         [2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   81,  153, 2502,\n",
       "           132,  161,   53,  558,   18,    1,    2,    2,    2,    2]])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the input that our model will receive, together with the outcome it will be measured against."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will build is a canonical encoder-decoder model. On the encoder side, a representation of the input is created, and on the decoder side this representation is used to generate a sequence of tokens in an autoregressive way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is a stack of N transformer layers, each of which is composed of a self-attention layer with a dense layer, including a residual connection. Let's start by defining the general Encoder:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding takes a sequence of token ids and converts it into a sequence of n-dimensional vector representations of each token. Both the encoder and decoder have separate embeddings, since they deal with separate languages.\n",
    "\n",
    "We will use the pytorch Embedding [https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model=512, n_vocab=20000):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this looks for our example batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10.7418, -18.5532, -24.5317,  ..., -16.8611, -28.6475,  -9.6731],\n",
       "         [-29.7550,  -7.0559,   9.6194,  ..., -16.2453, -19.8846, -17.7630],\n",
       "         [  7.4113,   2.9326, -20.6058,  ..., -30.7097, -10.0630,   3.7162],\n",
       "         ...,\n",
       "         [ 12.4295, -35.7591,  -2.5741,  ...,  32.7473,  57.2127,  10.5758],\n",
       "         [ 12.4295, -35.7591,  -2.5741,  ...,  32.7473,  57.2127,  10.5758],\n",
       "         [ 12.4295, -35.7591,  -2.5741,  ...,  32.7473,  57.2127,  10.5758]],\n",
       "\n",
       "        [[-10.7418, -18.5532, -24.5317,  ..., -16.8611, -28.6475,  -9.6731],\n",
       "         [-14.1038, -21.0192,  48.2898,  ...,   1.6452,  18.7295,  -5.7033],\n",
       "         [-69.6844,  60.1925, -21.8730,  ...,   0.7527,   0.4434, -19.8733],\n",
       "         ...,\n",
       "         [  1.6627,  14.5584,  -1.6639,  ...,   0.5623, -43.7432,   3.2204],\n",
       "         [  5.0090,  -6.6878, -22.5054,  ...,   3.4984,  -7.9396, -41.7292],\n",
       "         [ -1.5115,  11.6709,   7.6318,  ...,  -5.2615,   7.4185,   2.5457]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_embedding_output = Embeddings()(example_batch\n",
    "['src_input_ids'])\n",
    "example_embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_embedding_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted each token in the batch into a vector of dimension MODEL_DIM. The first rank of the tensor represents the batch no, the second is the token no and the third is the vector coefficient of the embedding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tell the model which position each token is in, the token embeddings have to be augmented with position information. We will use a common method that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model=512, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-11.9354, -19.5036, -27.2575,  ..., -17.6234, -31.8306,  -9.6368],\n",
       "         [-32.1261,  -7.2396,  11.6014,  ..., -16.9392, -22.0939, -18.6256],\n",
       "         [  9.2451,   2.7960, -21.8548,  ..., -33.0108, -11.1809,   5.2402],\n",
       "         ...,\n",
       "         [ 13.9771,  -0.0000,  -3.4132,  ...,  37.4970,  63.5718,  12.8620],\n",
       "         [ 14.8249, -39.2789,  -2.3832,  ...,  37.4970,  63.5719,  12.8620],\n",
       "         [ 14.7401, -40.3409,  -1.7637,  ...,  37.4970,  63.5720,  12.8620]],\n",
       "\n",
       "        [[-11.9354, -19.5036, -27.2575,  ..., -17.6234, -31.8306,  -9.6368],\n",
       "         [-14.7359, -22.7543,  54.5685,  ...,   2.9391,  20.8107,  -5.2259],\n",
       "         [-76.4167,   0.0000, -23.2629,  ...,   1.9474,   0.4929, -20.9703],\n",
       "         ...,\n",
       "         [  2.0140,  17.2745,  -2.4018,  ...,   1.7359, -48.6013,   4.6894],\n",
       "         [  6.5799,  -6.9775,  -0.0000,  ...,   4.9982,  -8.8195, -45.2547],\n",
       "         [ -0.7499,   0.0000,   0.0000,  ...,  -4.7350,   8.2452,   3.9397]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pos_enc_output = PositionalEncoding()(example_embedding_output)\n",
    "example_pos_enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pos_enc_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the tensor is the same as before, since all that has been added is some positional information to each dimension of each token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have transformed each input tokens into a vector representing that token, via an embedding, and added positional information, via the positional encoder. Now, the role of the encoder is to represent the input in such a way that the decoder can make best use of it. In order to do this, each layer of the encoder \"shares\" semantic information between tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Headed Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-headed attention is the mechanism by which this sharing takes place. It allows each token to learn how to \"pay attention\" to other tokens, and gradually assimilate their semantic meaning into their representations. Since the model has to deal with sequences of varying length, the attention mechanism has to learn how to first assess the relationship between tokens, and then how to share their representations i.e. it cannot say \"mix token 1 with token 3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(2) == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output of the multi-headed attention we need to feed it some input. We can feed it separate values for Q, K, and V. For the encoder, these will all be the representations of the tokens, hence the term \"self-attention\". We also input the attention mask, since we need to tell the attention layer to ignore any tokens that represent padding, coming from our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "         5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "        [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "          112, 7402,  170,  583, 9528,   90,  246, 1028,   18,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5848e+01,  3.4616e+00,  6.4872e+00,  ..., -3.3008e+00,\n",
       "           1.1372e-01,  5.3793e+00],\n",
       "         [ 6.5011e+00,  1.5313e+01,  2.6866e+00,  ..., -3.4565e+00,\n",
       "          -7.6228e+00, -2.3052e+00],\n",
       "         [ 4.1317e+00, -2.0901e+00, -8.5896e-01,  ...,  1.1223e+01,\n",
       "           7.2100e+00,  1.4491e+00],\n",
       "         ...,\n",
       "         [ 6.9053e+00, -6.7535e+00, -4.3447e+00,  ..., -6.9228e+00,\n",
       "          -1.4999e+00, -1.2341e+01],\n",
       "         [-1.0241e+00, -1.0723e+01, -9.4821e+00,  ...,  3.4602e+00,\n",
       "          -7.1168e+00, -1.4010e+01],\n",
       "         [ 5.1978e-01, -3.0343e+00, -5.6894e+00,  ..., -1.5615e+00,\n",
       "          -1.0337e+01,  5.8480e+00]],\n",
       "\n",
       "        [[-1.5500e+01,  3.6269e+00,  7.3738e+00,  ..., -7.0873e+00,\n",
       "           8.4160e+00,  1.9761e+00],\n",
       "         [-3.5880e-01,  2.7700e+00, -1.3743e+01,  ..., -3.7603e+00,\n",
       "           5.8246e+00,  2.0918e+01],\n",
       "         [-5.8391e+00,  2.0329e+01, -4.8123e+00,  ...,  1.2425e+01,\n",
       "           1.5544e+00, -6.5014e+00],\n",
       "         ...,\n",
       "         [ 1.5815e-01, -7.7162e+00,  7.0400e+00,  ..., -4.6268e+00,\n",
       "           8.9525e+00, -8.9073e-01],\n",
       "         [ 7.3792e+00, -1.1192e+01, -2.1101e+01,  ...,  7.0332e+00,\n",
       "          -1.1621e+01, -5.5976e+00],\n",
       "         [ 1.4242e+00, -8.8167e+00, -1.9746e-03,  ..., -1.8774e+01,\n",
       "          -2.1109e+00, -7.0856e+00]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_output = MultiHeadedAttention(h=8, d_model=MODEL_DIM, dropout=0.1)(\n",
    "    example_pos_enc_output,\n",
    "    example_pos_enc_output, \n",
    "    example_pos_enc_output,\n",
    "    example_batch['src_attention_mask']\n",
    ")\n",
    "example_attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-layer Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1788e+01, -1.9641e+01, -2.7146e+01,  ..., -1.7288e+01,\n",
       "          -3.1859e+01, -8.9730e+00],\n",
       "         [-3.2267e+01, -7.1726e+00,  1.1578e+01,  ..., -1.6820e+01,\n",
       "          -2.2094e+01, -1.8394e+01],\n",
       "         [ 9.0569e+00,  2.8178e+00, -2.1855e+01,  ..., -3.2908e+01,\n",
       "          -1.1329e+01,  5.4037e+00],\n",
       "         ...,\n",
       "         [ 1.3910e+01, -5.9531e-02, -3.4236e+00,  ...,  3.7680e+01,\n",
       "           6.3612e+01,  1.3031e+01],\n",
       "         [ 1.4747e+01, -3.9312e+01, -2.3949e+00,  ...,  3.7624e+01,\n",
       "           6.3644e+01,  1.3096e+01],\n",
       "         [ 1.4697e+01, -4.0440e+01, -1.6874e+00,  ...,  3.7690e+01,\n",
       "           6.3671e+01,  1.3098e+01]],\n",
       "\n",
       "        [[-1.1593e+01, -1.9858e+01, -2.7173e+01,  ..., -1.7236e+01,\n",
       "          -3.1707e+01, -9.0266e+00],\n",
       "         [-1.4586e+01, -2.2747e+01,  5.4654e+01,  ...,  3.1466e+00,\n",
       "           2.0957e+01, -5.2655e+00],\n",
       "         [-7.6384e+01,  0.0000e+00, -2.3228e+01,  ...,  2.4682e+00,\n",
       "           6.0220e-01, -2.0970e+01],\n",
       "         ...,\n",
       "         [ 2.1270e+00,  1.7406e+01, -2.5977e+00,  ...,  1.7359e+00,\n",
       "          -4.8313e+01,  4.7000e+00],\n",
       "         [ 6.3965e+00, -7.1619e+00, -2.2976e-01,  ...,  5.1851e+00,\n",
       "          -8.6387e+00, -4.5131e+01],\n",
       "         [-7.8308e-01,  2.5506e-03, -5.9056e-02,  ..., -4.3640e+00,\n",
       "           8.0263e+00,  4.0405e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_sublayer_output = SublayerConnection(size=MODEL_DIM, dropout=0.1)(\n",
    "    example_pos_enc_output, \n",
    "    lambda x: MultiHeadedAttention(h=8, d_model=MODEL_DIM, dropout=0.1)(x, x, x, example_batch['src_attention_mask'])\n",
    ")\n",
    "example_attn_sublayer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_attn_sublayer_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-Wise Feed-forward Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token vector is then fed through an identical densely connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.8549,   0.1118,  -1.6097,  ...,  -1.1000,  -2.7858,   5.0261],\n",
       "         [  3.2303,  -3.5483,  -1.9076,  ...,  -4.4952,  -5.0886,   8.3172],\n",
       "         [  4.7101,   2.1633,   0.6520,  ...,   1.3836,   1.2963,  12.9888],\n",
       "         ...,\n",
       "         [  8.9456,   1.2445,  -0.4396,  ...,  -4.1853,   8.7339,   4.8806],\n",
       "         [  6.0206,   5.1609,   1.1854,  ...,  -7.8664,   9.6261,   5.6694],\n",
       "         [  7.4900,   6.4613,  -2.6274,  ...,  -3.1340,   7.8336,   6.0030]],\n",
       "\n",
       "        [[ -3.1191,   3.0587,  -1.2532,  ...,  -1.5697,   2.1519,   4.7717],\n",
       "         [ -2.5520,   2.5238,  -3.8402,  ..., -11.1359,  11.8362,   6.0863],\n",
       "         [  5.3625,  -2.4541,   2.3027,  ...,  -9.8394,   2.3857,   1.0974],\n",
       "         ...,\n",
       "         [  0.4157,   1.8016,  -7.2268,  ...,  -7.8388,   6.0038,   7.1653],\n",
       "         [ 10.9670,  -2.8875,   1.2094,  ...,   8.0200,   4.0866,   4.5787],\n",
       "         [ 11.5248,  -1.4691,  -7.9380,  ...,  -4.2281,  -5.6999,   2.7419]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ffn_output = PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)(\n",
    "    example_attn_sublayer_output\n",
    ")\n",
    "example_ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ffn_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also wrapped in a residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1672e+01, -1.9646e+01, -2.6915e+01,  ..., -1.6866e+01,\n",
       "          -3.1626e+01, -8.8061e+00],\n",
       "         [-3.2235e+01, -7.1657e+00,  1.1565e+01,  ..., -1.6820e+01,\n",
       "          -2.2188e+01, -1.8719e+01],\n",
       "         [ 9.0568e+00,  2.9613e+00, -2.2330e+01,  ..., -3.2912e+01,\n",
       "          -1.1085e+01,  5.3018e+00],\n",
       "         ...,\n",
       "         [ 1.4263e+01, -5.9531e-02, -3.7806e+00,  ...,  3.7680e+01,\n",
       "           6.3945e+01,  1.2929e+01],\n",
       "         [ 1.4798e+01, -3.9192e+01, -2.6850e+00,  ...,  3.7624e+01,\n",
       "           6.3987e+01,  1.3220e+01],\n",
       "         [ 1.4858e+01, -4.0123e+01, -1.9649e+00,  ...,  3.7987e+01,\n",
       "           6.4030e+01,  1.3245e+01]],\n",
       "\n",
       "        [[-1.1544e+01, -1.9902e+01, -2.7173e+01,  ..., -1.6947e+01,\n",
       "          -3.1506e+01, -8.8545e+00],\n",
       "         [-1.4717e+01, -2.2579e+01,  5.3989e+01,  ...,  3.2638e+00,\n",
       "           2.1064e+01, -5.5290e+00],\n",
       "         [-7.6252e+01,  2.0399e-01, -2.3151e+01,  ...,  2.6145e+00,\n",
       "           6.2997e-01, -2.0860e+01],\n",
       "         ...,\n",
       "         [ 2.2762e+00,  1.7518e+01, -2.8515e+00,  ...,  2.1500e+00,\n",
       "          -4.8388e+01,  4.6755e+00],\n",
       "         [ 6.5565e+00, -7.1619e+00, -9.6296e-02,  ...,  5.1851e+00,\n",
       "          -8.4958e+00, -4.5199e+01],\n",
       "         [-6.4220e-01, -2.0042e-01, -5.2950e-01,  ..., -4.0927e+00,\n",
       "           8.2535e+00,  4.1428e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sublayer_ffn_output = SublayerConnection(size=MODEL_DIM, dropout=0.1)(\n",
    "    example_attn_sublayer_output, \n",
    "    lambda x: PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)(x)\n",
    ")\n",
    "example_sublayer_ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sublayer_ffn_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've reached the end of our first encoder layer. Putting it all together each encoder layer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the full encoder is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7675e-01, -8.5384e-01, -1.1210e+00,  ..., -6.9738e-01,\n",
       "          -1.3103e+00, -3.6737e-01],\n",
       "         [-1.4606e+00, -3.2886e-01,  4.1182e-01,  ..., -7.8795e-01,\n",
       "          -1.0392e+00, -8.1147e-01],\n",
       "         [ 4.4371e-01,  1.8860e-01, -7.9598e-01,  ..., -1.3357e+00,\n",
       "          -4.8368e-01,  2.5915e-01],\n",
       "         ...,\n",
       "         [ 6.1511e-01, -2.4969e-04, -1.6428e-01,  ...,  1.5807e+00,\n",
       "           2.6607e+00,  5.5483e-01],\n",
       "         [ 6.4504e-01, -1.6256e+00, -1.4088e-01,  ...,  1.5224e+00,\n",
       "           2.5851e+00,  5.1944e-01],\n",
       "         [ 6.4625e-01, -1.6383e+00, -9.0536e-02,  ...,  1.5085e+00,\n",
       "           2.5626e+00,  5.3021e-01]],\n",
       "\n",
       "        [[-4.7039e-01, -8.0318e-01, -1.1159e+00,  ..., -6.8493e-01,\n",
       "          -1.2943e+00, -3.6683e-01],\n",
       "         [-6.6198e-01, -9.9660e-01,  2.0223e+00,  ...,  5.2900e-02,\n",
       "           7.4259e-01, -3.0415e-01],\n",
       "         [-3.2112e+00, -3.9775e-02, -9.5455e-01,  ...,  6.1165e-02,\n",
       "           1.5811e-02, -9.0908e-01],\n",
       "         ...,\n",
       "         [ 6.3719e-02,  6.8256e-01, -1.5554e-01,  ...,  3.3290e-02,\n",
       "          -2.0439e+00,  1.7466e-01],\n",
       "         [ 2.5637e-01, -2.1749e-01,  4.0655e-02,  ...,  1.8133e-01,\n",
       "          -3.4885e-01, -1.9233e+00],\n",
       "         [-4.6076e-02, -3.0489e-02, -5.1459e-02,  ..., -2.2377e-01,\n",
       "           3.6926e-01,  1.4014e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=2, d_model=MODEL_DIM)\n",
    "ff = PositionwiseFeedForward(d_model=MODEL_DIM, d_ff=2048, dropout=0.1)\n",
    "dropout = 0.1\n",
    "example_encoder_output = Encoder(\n",
    "    layer=EncoderLayer(\n",
    "        size=MODEL_DIM, \n",
    "        self_attn=c(attn), \n",
    "        feed_forward=c(ff), \n",
    "        dropout=0.1), \n",
    "    N=2)(example_pos_enc_output, example_batch['src_attention_mask'])\n",
    "example_encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_encoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the decoder is:\n",
    "1. The output of the encoder\n",
    "2. The previous tokens in the sequence up to that point\n",
    "3. The source mask showing which source tokens the decoder can pay attention to\n",
    "4. The target mask showing which target tokens the decoder can pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([[   0,   45,  741,  131,  131,  167,  813, 2058,  112,  671, 8549,  435,\n",
       "          5596, 1245,   18,    1,    2,    2,    2,    2,    2,    2],\n",
       "         [   0, 4402, 1068,  250, 2665,  398,  202,  697, 2062,  768,   16,  136,\n",
       "           112, 7402,  170,  583, 9528,   90,  246, 1028,   18,    1]]),\n",
       " 'src_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'tgt_input_ids': tensor([[   0, 1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,\n",
       "           583,  326,  682, 1735,   18,    1,    2,    2,    2,    2],\n",
       "         [   0, 2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   81,  153,\n",
       "          2502,  132,  161,   53,  558,   18,    2,    2,    2,    2]]),\n",
       " 'tgt_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " 'tgt_output_ids': tensor([[1070,   63,  195,  196,   53, 1577,   47, 1842,   11,   56, 9015,  583,\n",
       "           326,  682, 1735,   18,    1,    2,    2,    2,    2,    2],\n",
       "         [2704, 1213, 2214,  432, 3405, 1984, 2936,   39, 7446,   81,  153, 2502,\n",
       "           132,  161,   53,  558,   18,    1,    2,    2,    2,    2]])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-13.4112,   7.1272,   4.7381,  ...,   0.0000,   0.0000,   5.6412],\n",
       "         [-13.6782,   6.9464, -21.1748,  ..., -29.7911,  16.4421,  -1.3626],\n",
       "         [  9.9126,   6.8113,  42.7099,  ...,  -8.9077,  25.2430, -25.2969],\n",
       "         ...,\n",
       "         [ 21.1316,  -0.0000, -12.8013,  ...,  -0.0000, -20.2269,   1.8317],\n",
       "         [  0.0000, -36.5859, -11.7713,  ..., -11.2292, -20.2268,   1.8317],\n",
       "         [ 21.8947, -37.6479, -11.1518,  ..., -11.2292, -20.2267,   1.8317]],\n",
       "\n",
       "        [[-13.4112,   7.1272,   4.7381,  ...,  16.0348,   0.4709,   5.6412],\n",
       "         [ -9.5638,  14.3176, -14.5953,  ...,   0.2070, -35.2907,  19.7233],\n",
       "         [-14.9374, -49.6283,  10.3585,  ..., -31.4959,  12.3004,   0.0000],\n",
       "         ...,\n",
       "         [ 21.1316, -35.9407, -12.8013,  ..., -11.2292, -20.2269,   1.8317],\n",
       "         [ 21.9795, -36.5859, -11.7713,  ..., -11.2292, -20.2268,   1.8317],\n",
       "         [ 21.8947, -37.6479, -11.1518,  ..., -11.2292, -20.2267,   1.8317]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tgt_embedding_output = Embeddings(512, VOCAB_SIZE)(example_batch['tgt_input_ids'])\n",
    "example_tgt_pos_enc_output = PositionalEncoding(512,0.1)(example_tgt_embedding_output)\n",
    "example_tgt_pos_enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tgt_pos_enc_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.2955, -10.7711,   2.6980,  ...,   3.7034,   8.0004,  -3.6077],\n",
       "         [ 11.1394, -11.7099,  24.3704,  ...,   2.4531,   6.8912,  -6.0044],\n",
       "         [  0.0283,   8.6027, -14.1181,  ...,   4.5193,  -2.7656,  12.5710],\n",
       "         ...,\n",
       "         [  2.5822,  -3.9382,   0.5822,  ...,  11.2627,   4.7259,  -5.3718],\n",
       "         [  2.1448,  14.8411,  -3.4153,  ...,   7.0794,   0.6785,  -1.5732],\n",
       "         [ 15.4648,  18.4373, -13.1424,  ...,  19.3747,   3.9657,  -5.6436]],\n",
       "\n",
       "        [[  0.6216,  -5.1710,  -6.3400,  ...,   2.4746,   7.7962,  -7.3516],\n",
       "         [ -2.3201,  -4.2860, -11.7811,  ...,  -2.4208,   0.0779,   0.2381],\n",
       "         [ 17.7032, -11.5069,   2.7769,  ...,  -7.9180,  -7.9924,   9.1242],\n",
       "         ...,\n",
       "         [ 14.7434,  -1.3001,   6.6742,  ...,   4.6925,  -2.3997,   2.8855],\n",
       "         [ 14.6501,  -3.4187,   6.9774,  ...,   6.5574, -10.0707,   4.2343],\n",
       "         [  9.1968,  -7.7419,   4.7432,  ...,  -0.9139, -10.7522,   8.3900]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadedAttention(h=8, d_model=512)(\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_tgt_pos_enc_output,\n",
    "    example_batch['tgt_attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-13.3243,   7.2067,   4.7191,  ...,   0.1770,   0.4830,   6.2686],\n",
       "         [-13.8562,   6.7913, -21.2996,  ..., -29.4380,  16.7292,  -1.2595],\n",
       "         [  9.8920,   6.6326,  42.6286,  ...,  -8.6700,  25.4093, -25.3929],\n",
       "         ...,\n",
       "         [ 20.9830,   0.2361, -12.7844,  ...,   0.3443, -20.3158,   2.1160],\n",
       "         [ -0.1887, -36.1978, -11.9729,  ..., -11.2234, -20.3709,   2.0473],\n",
       "         [ 21.8522, -37.4589, -11.2534,  ..., -10.7406, -20.6673,   2.1370]],\n",
       "\n",
       "        [[-13.0859,   7.1533,   4.8148,  ...,  16.5620,   0.5678,   6.6535],\n",
       "         [ -9.1321,  14.4113, -14.1881,  ...,   0.5718, -36.0848,  19.8146],\n",
       "         [-14.9012, -49.1435,  10.3008,  ..., -30.9242,  12.6328,   0.7469],\n",
       "         ...,\n",
       "         [ 21.4469, -35.7159, -13.0199,  ..., -10.8138, -20.5707,   2.1509],\n",
       "         [ 21.9492, -36.1538, -11.5344,  ..., -10.9173, -20.5362,   2.2667],\n",
       "         [ 21.9387, -37.5437, -11.0821,  ..., -10.9419, -20.4520,   2.0873]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=8, d_model=512)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "position = PositionalEncoding(d_model=512, dropout=0.1)\n",
    "DecoderLayer(512, c(attn), c(attn), c(ff), dropout=0.1)(\n",
    "    x=example_tgt_pos_enc_output,\n",
    "    memory=example_encoder_output,\n",
    "    src_mask=example_batch['src_attention_mask'],\n",
    "    tgt_mask=example_batch['tgt_attention_mask']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h=8, d_model=512)\n",
    "ff = PositionwiseFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "position = PositionalEncoding(d_model=512, dropout=0.1)\n",
    "example_decoder_output = Decoder(\n",
    "    layer=DecoderLayer(512, c(attn), c(attn), c(ff), dropout=0.1),\n",
    "    N=2\n",
    ")(x=example_tgt_pos_enc_output,\n",
    "    memory=example_encoder_output,\n",
    "    src_mask=example_batch['src_attention_mask'],\n",
    "    tgt_mask=example_batch['tgt_attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8735e-01,  2.3710e-01,  1.4858e-01,  ..., -2.6921e-02,\n",
       "           1.1675e-02,  1.9538e-01],\n",
       "         [-6.9158e-01,  1.9578e-01, -9.3966e-01,  ..., -1.2920e+00,\n",
       "           5.9979e-01, -1.2577e-01],\n",
       "         [ 4.3333e-01,  2.8341e-01,  1.7773e+00,  ..., -3.6972e-01,\n",
       "           1.0646e+00, -1.0376e+00],\n",
       "         ...,\n",
       "         [ 7.8984e-01, -4.9802e-02, -6.4208e-01,  ..., -2.1265e-02,\n",
       "          -9.3646e-01, -2.8173e-02],\n",
       "         [-7.4588e-02, -1.6112e+00, -5.7532e-01,  ..., -4.8605e-01,\n",
       "          -9.2021e-01, -2.5805e-02],\n",
       "         [ 8.6689e-01, -1.6594e+00, -5.5446e-01,  ..., -5.1623e-01,\n",
       "          -9.2020e-01, -2.1505e-03]],\n",
       "\n",
       "        [[-5.9821e-01,  2.2719e-01,  1.5098e-01,  ...,  6.2943e-01,\n",
       "           1.7150e-02,  1.9239e-01],\n",
       "         [-3.8294e-01,  5.8034e-01, -5.6960e-01,  ..., -5.8507e-03,\n",
       "          -1.4720e+00,  8.2239e-01],\n",
       "         [-7.1198e-01, -2.1967e+00,  3.9935e-01,  ..., -1.4091e+00,\n",
       "           4.7863e-01, -7.9248e-02],\n",
       "         ...,\n",
       "         [ 8.4954e-01, -1.6061e+00, -6.5559e-01,  ..., -5.0183e-01,\n",
       "          -9.6694e-01, -2.8638e-02],\n",
       "         [ 8.8742e-01, -1.6262e+00, -5.9930e-01,  ..., -5.2488e-01,\n",
       "          -9.5096e-01, -1.4427e-02],\n",
       "         [ 8.6820e-01, -1.6570e+00, -5.5806e-01,  ..., -5.0875e-01,\n",
       "          -9.4436e-01, -1.8896e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_decoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to take the output of the decoder and predict the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator_output = Generator(d_model=512, vocab=VOCAB_SIZE)(example_decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 10000])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_generator_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the generator takes in the decoder output for each step, and maps the decoder vector representation for each token to a probability for *all* tokens in the sequence, not just the last one. For example, the first sequence, first token output log probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.5265, -9.7819, -9.7393,  ..., -8.5086, -9.4319, -7.8582],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_generator_output[0, 0, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely token is therefore the one with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8799)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ind = example_generator_output[0, 0, :].argmax()\n",
    "vocab_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chynrychioli'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "welsh_tokenizer.decode(vocab_ind)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the full model together gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s9r705790jl1sf67t3r42qgc0000gp/T/ipykernel_16211/2289673833.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "example_model = make_model(VOCAB_SIZE, VOCAB_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s9r705790jl1sf67t3r42qgc0000gp/T/ipykernel_16211/2289673833.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1169, -0.5819,  0.0130,  ..., -0.8328,  1.0758,  0.4566],\n",
       "         [ 0.2189, -1.0932,  1.3029,  ..., -1.0174,  1.1675,  0.2970],\n",
       "         [ 0.6341, -0.4515,  0.7189,  ..., -0.2266,  0.7938,  0.1137],\n",
       "         ...,\n",
       "         [ 1.1180, -0.5693,  0.7974,  ..., -0.9358,  0.9848,  0.0183],\n",
       "         [ 0.4512,  0.0294, -0.0631,  ..., -1.0847,  1.5236,  0.4952],\n",
       "         [ 1.8831, -1.0368,  1.4201,  ..., -0.0585,  0.3711, -0.3482]],\n",
       "\n",
       "        [[ 0.5812,  0.4304, -0.5816,  ..., -1.6874,  0.9159,  0.1194],\n",
       "         [ 1.1245, -0.3786, -0.2071,  ..., -1.2755,  0.7649, -0.2972],\n",
       "         [ 1.8569, -0.4656,  0.8226,  ..., -0.8058,  1.1237, -0.2432],\n",
       "         ...,\n",
       "         [ 1.6085,  0.4306, -0.0887,  ..., -1.4484,  0.9135,  0.4617],\n",
       "         [ 2.1419, -0.2611,  0.2290,  ..., -1.1957,  0.4064, -0.2649],\n",
       "         [ 2.0685, -1.0459,  1.5858,  ..., -0.9481,  0.7386, -0.2194]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model_output = make_model(VOCAB_SIZE, VOCAB_SIZE, 2)(\n",
    "    example_batch['src_input_ids'], example_batch['tgt_input_ids'], example_batch['src_attention_mask'], example_batch['tgt_attention_mask'])\n",
    "example_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 512])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of the model doesn't use the Generator?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "#import GPUtil\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model we need to decide on a few training parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_optimizer = torch.optim.Adam(\n",
    "        example_model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lr_scheduler = LambdaLR(\n",
    "    optimizer=example_optimizer,\n",
    "    lr_lambda=lambda step: rate(\n",
    "        step, 512, factor=1, warmup=100\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(corpus['train'], batch_size=64, collate_fn=collate_batch)\n",
    "val_dataloader = torch.utils.data.DataLoader(corpus['val'], batch_size=64, collate_fn=collate_batch)\n",
    "test_dataloader = torch.utils.data.DataLoader(corpus['test'], batch_size=64, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "): \n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch['src_input_ids'],\n",
    "            batch['tgt_input_ids'], \n",
    "            batch['src_attention_mask'],\n",
    "            batch['tgt_attention_mask']\n",
    "        )\n",
    "        ntokens = (batch['tgt_output_ids'] != 2).data.sum()\n",
    "        loss, loss_node = loss_compute(out, batch['tgt_output_ids'], ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch['src_input_ids'].shape[0]\n",
    "            train_state.tokens += ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "        tokens += ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   6.23 | Tokens / Sec:   352.2 | Learning Rate: 9.6e-07\n",
      "Epoch Step:     41 | Accumulation Step:  42 | Loss:   6.27 | Tokens / Sec:   490.1 | Learning Rate: 9.5e-07\n",
      "Epoch Step:     81 | Accumulation Step:  82 | Loss:   6.33 | Tokens / Sec:   467.9 | Learning Rate: 9.4e-07\n",
      "Epoch Step:    121 | Accumulation Step: 122 | Loss:   6.28 | Tokens / Sec:   514.9 | Learning Rate: 9.3e-07\n",
      "Epoch Step:    161 | Accumulation Step: 162 | Loss:   6.17 | Tokens / Sec:   555.5 | Learning Rate: 9.2e-07\n",
      "Epoch Step:    201 | Accumulation Step: 202 | Loss:   6.30 | Tokens / Sec:   535.7 | Learning Rate: 9.1e-07\n",
      "Epoch Step:    241 | Accumulation Step: 242 | Loss:   6.30 | Tokens / Sec:   552.5 | Learning Rate: 9.1e-07\n",
      "Epoch Step:    281 | Accumulation Step: 282 | Loss:   6.36 | Tokens / Sec:   543.7 | Learning Rate: 9.0e-07\n",
      "Epoch Step:    321 | Accumulation Step: 322 | Loss:   6.21 | Tokens / Sec:   538.9 | Learning Rate: 8.9e-07\n",
      "Epoch Step:    361 | Accumulation Step: 362 | Loss:   6.27 | Tokens / Sec:   582.3 | Learning Rate: 8.8e-07\n",
      "Epoch Step:    401 | Accumulation Step: 402 | Loss:   6.25 | Tokens / Sec:   482.4 | Learning Rate: 8.8e-07\n",
      "Epoch Step:    441 | Accumulation Step: 442 | Loss:   6.01 | Tokens / Sec:   494.6 | Learning Rate: 8.7e-07\n",
      "Epoch Step:    481 | Accumulation Step: 482 | Loss:   6.15 | Tokens / Sec:   505.3 | Learning Rate: 8.6e-07\n",
      "Epoch Step:    521 | Accumulation Step: 522 | Loss:   6.35 | Tokens / Sec:   494.6 | Learning Rate: 8.6e-07\n",
      "Epoch Step:    561 | Accumulation Step: 562 | Loss:   6.26 | Tokens / Sec:   463.7 | Learning Rate: 8.5e-07\n",
      "Epoch Step:    601 | Accumulation Step: 602 | Loss:   6.25 | Tokens / Sec:   509.6 | Learning Rate: 8.4e-07\n",
      "Epoch Step:    641 | Accumulation Step: 642 | Loss:   6.09 | Tokens / Sec:   471.7 | Learning Rate: 8.4e-07\n",
      "Epoch Step:    681 | Accumulation Step: 682 | Loss:   6.14 | Tokens / Sec:   503.1 | Learning Rate: 8.3e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_epoch(\n\u001b[1;32m      2\u001b[0m     data_iter\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      3\u001b[0m     model\u001b[39m=\u001b[39;49mexample_model,\n\u001b[1;32m      4\u001b[0m     loss_compute\u001b[39m=\u001b[39;49mSimpleLossCompute(example_model\u001b[39m.\u001b[39;49mgenerator, LabelSmoothing(\n\u001b[1;32m      5\u001b[0m         size\u001b[39m=\u001b[39;49mVOCAB_SIZE, padding_idx\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, smoothing\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m\n\u001b[1;32m      6\u001b[0m     )),\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m=\u001b[39;49mexample_optimizer,\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[39m=\u001b[39;49mexample_lr_scheduler,\n\u001b[1;32m      9\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[149], line 25\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute, optimizer, scheduler, mode, accum_iter, train_state)\u001b[0m\n\u001b[1;32m     18\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(\n\u001b[1;32m     19\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39msrc_input_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mtgt_input_ids\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     21\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39msrc_attention_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     22\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mtgt_attention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m ntokens \u001b[39m=\u001b[39m (batch[\u001b[39m'\u001b[39m\u001b[39mtgt_output_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> 25\u001b[0m loss, loss_node \u001b[39m=\u001b[39m loss_compute(out, batch[\u001b[39m'\u001b[39;49m\u001b[39mtgt_output_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], ntokens)\n\u001b[1;32m     26\u001b[0m \u001b[39m# loss_node = loss_node / accum_iter\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain+log\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[138], line 9\u001b[0m, in \u001b[0;36mSimpleLossCompute.__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, y, norm):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator(x)\n\u001b[1;32m     10\u001b[0m     sloss \u001b[39m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(\n\u001b[1;32m     12\u001b[0m             x\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), y\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m         \u001b[39m/\u001b[39m norm\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m sloss\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m norm, sloss\n",
      "File \u001b[0;32m~/Repos/mdpead.github.io/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[125], line 8\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlog_softmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_epoch(\n",
    "    data_iter=train_dataloader,\n",
    "    model=example_model,\n",
    "    loss_compute=SimpleLossCompute(example_model.generator, LabelSmoothing(\n",
    "        size=VOCAB_SIZE, padding_idx=2, smoothing=0.1\n",
    "    )),\n",
    "    optimizer=example_optimizer,\n",
    "    scheduler=example_lr_scheduler,\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    config\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    #torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = 2\n",
    "    d_model = 512\n",
    "    model = make_model(VOCAB_SIZE, VOCAB_SIZE, N=6)\n",
    "    #model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=VOCAB_SIZE, padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    # train_dataloader, valid_dataloader = create_dataloaders(\n",
    "    #     gpu,\n",
    "    #     vocab_src,\n",
    "    #     vocab_tgt,\n",
    "    #     spacy_de,\n",
    "    #     spacy_en,\n",
    "    #     batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "    #     max_padding=config[\"max_padding\"],\n",
    "    #     is_distributed=is_distributed,\n",
    "    # )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            data_iter(),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        #GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            data_iter(),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    train_worker(\n",
    "        0, config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"distributed\": False,\n",
    "    \"num_epochs\": 1,\n",
    "    \"accum_iter\": 10,\n",
    "    \"base_lr\": 1.0,\n",
    "    \"max_padding\": 72,\n",
    "    \"warmup\": 3000,\n",
    "    \"file_prefix\": \"english_welsh_model_\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train worker process using GPU: 0 for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/s9r705790jl1sf67t3r42qgc0000gp/T/ipykernel_17376/2289673833.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] Epoch 0 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.98 | Tokens / Sec:   193.3 | Learning Rate: 5.4e-07\n",
      "[GPU0] Epoch 0 Validation ====\n",
      "(tensor(7.9762), <__main__.TrainState object at 0x7f97cdb98880>)\n"
     ]
    }
   ],
   "source": [
    "train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e332c74c34b9658bfa6d95a671b821ca0df11635f40c2cc5e198c5900786c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
